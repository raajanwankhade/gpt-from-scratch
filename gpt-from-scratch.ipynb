{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GPT From Scratch\nInspired by Andrej Karpathy's video lecture: https://www.youtube.com/watch?v=kCc8FmEb1nY","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:24:34.051793Z","iopub.execute_input":"2025-05-13T17:24:34.052497Z","iopub.status.idle":"2025-05-13T17:24:34.056199Z","shell.execute_reply.started":"2025-05-13T17:24:34.052471Z","shell.execute_reply":"2025-05-13T17:24:34.055611Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"ds = load_dataset(\"roneneldan/TinyStories\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:24:34.064418Z","iopub.execute_input":"2025-05-13T17:24:34.064596Z","iopub.status.idle":"2025-05-13T17:24:35.531974Z","shell.execute_reply.started":"2025-05-13T17:24:34.064582Z","shell.execute_reply":"2025-05-13T17:24:35.531317Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:24:35.533182Z","iopub.execute_input":"2025-05-13T17:24:35.533426Z","iopub.status.idle":"2025-05-13T17:24:35.538182Z","shell.execute_reply.started":"2025-05-13T17:24:35.533404Z","shell.execute_reply":"2025-05-13T17:24:35.537501Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 2119719\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 21990\n    })\n})"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"# combining all the text that we have\ntext = \"\\n\".join(ds['train']['text'] + ds['validation']['text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:24:35.538806Z","iopub.execute_input":"2025-05-13T17:24:35.538985Z","iopub.status.idle":"2025-05-13T17:24:42.023087Z","shell.execute_reply.started":"2025-05-13T17:24:35.538970Z","shell.execute_reply":"2025-05-13T17:24:42.022513Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"print(\"#characters:\", len(text))\nprint(text[:1000])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:24:42.024641Z","iopub.execute_input":"2025-05-13T17:24:42.024918Z","iopub.status.idle":"2025-05-13T17:24:42.029114Z","shell.execute_reply.started":"2025-05-13T17:24:42.024888Z","shell.execute_reply":"2025-05-13T17:24:42.028399Z"}},"outputs":[{"name":"stdout","text":"#characters: 1921305229\nOne day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n\nTogether, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\nOnce upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\n\nOne day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. B\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"text = text[:1921300]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:24:42.029969Z","iopub.execute_input":"2025-05-13T17:24:42.030759Z","iopub.status.idle":"2025-05-13T17:24:42.385518Z","shell.execute_reply.started":"2025-05-13T17:24:42.030735Z","shell.execute_reply":"2025-05-13T17:24:42.384839Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"chars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))  # all unique characters\nprint(vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:24:42.386347Z","iopub.execute_input":"2025-05-13T17:24:42.386608Z","iopub.status.idle":"2025-05-13T17:24:42.418606Z","shell.execute_reply.started":"2025-05-13T17:24:42.386582Z","shell.execute_reply":"2025-05-13T17:24:42.417949Z"}},"outputs":[{"name":"stdout","text":"\n !\"$',-.012345678:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz¡¦©«±³»ÂÃâœ˜“”€™\n89\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# encoding characters to integers\nctoi = {char:i for i, char in enumerate(chars)}\nitoc = {i:char for i, char in enumerate(chars)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:24:42.419316Z","iopub.execute_input":"2025-05-13T17:24:42.419522Z","iopub.status.idle":"2025-05-13T17:24:42.435782Z","shell.execute_reply.started":"2025-05-13T17:24:42.419495Z","shell.execute_reply":"2025-05-13T17:24:42.435100Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# defining the encoding and decoding functions as lambda functions\nencode = lambda s: [ctoi[c] for c in s]\ndecode = lambda l: ''.join([itoc[i] for i in l])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:24:42.436478Z","iopub.execute_input":"2025-05-13T17:24:42.436703Z","iopub.status.idle":"2025-05-13T17:24:42.452977Z","shell.execute_reply.started":"2025-05-13T17:24:42.436679Z","shell.execute_reply":"2025-05-13T17:24:42.452423Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"print(encode(\"hello NITK\"))\nprint(decode(encode(\"hello NITK\")))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:24:42.453711Z","iopub.execute_input":"2025-05-13T17:24:42.453902Z","iopub.status.idle":"2025-05-13T17:24:42.468311Z","shell.execute_reply.started":"2025-05-13T17:24:42.453882Z","shell.execute_reply":"2025-05-13T17:24:42.467794Z"}},"outputs":[{"name":"stdout","text":"[54, 51, 58, 58, 61, 1, 34, 29, 40, 31]\nhello NITK\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"data = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:24:42.471108Z","iopub.execute_input":"2025-05-13T17:24:42.471354Z","iopub.status.idle":"2025-05-13T17:24:42.684089Z","shell.execute_reply.started":"2025-05-13T17:24:42.471335Z","shell.execute_reply":"2025-05-13T17:24:42.683471Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1921300]) torch.int64\ntensor([35, 60, 51,  1, 50, 47, 71,  6,  1, 47,  1, 58, 55, 66, 66, 58, 51,  1,\n        53, 55, 64, 58,  1, 60, 47, 59, 51, 50,  1, 32, 55, 58, 71,  1, 52, 61,\n        67, 60, 50,  1, 47,  1, 60, 51, 51, 50, 58, 51,  1, 55, 60,  1, 54, 51,\n        64,  1, 64, 61, 61, 59,  8,  1, 39, 54, 51,  1, 57, 60, 51, 69,  1, 55,\n        66,  1, 69, 47, 65,  1, 50, 55, 52, 52, 55, 49, 67, 58, 66,  1, 66, 61,\n         1, 62, 58, 47, 71,  1, 69, 55, 66, 54,  1, 55, 66,  1, 48, 51, 49, 47,\n        67, 65, 51,  1, 55, 66,  1, 69, 47, 65,  1, 65, 54, 47, 64, 62,  8,  1,\n        32, 55, 58, 71,  1, 69, 47, 60, 66, 51, 50,  1, 66, 61,  1, 65, 54, 47,\n        64, 51,  1, 66, 54, 51,  1, 60, 51, 51, 50, 58, 51,  1, 69, 55, 66, 54,\n         1, 54, 51, 64,  1, 59, 61, 59,  6,  1, 65, 61,  1, 65, 54, 51,  1, 49,\n        61, 67, 58, 50,  1, 65, 51, 69,  1, 47,  1, 48, 67, 66, 66, 61, 60,  1,\n        61, 60,  1, 54, 51, 64,  1, 65, 54, 55, 64, 66,  8,  0,  0, 32, 55, 58,\n        71,  1, 69, 51, 60, 66,  1, 66, 61,  1, 54, 51, 64,  1, 59, 61, 59,  1,\n        47, 60, 50,  1, 65, 47, 55, 50,  6,  1,  3, 33, 61, 59,  6,  1, 29,  1,\n        52, 61, 67, 60, 50,  1, 66, 54, 55, 65,  1, 60, 51, 51, 50, 58, 51,  8,\n         1, 23, 47, 60,  1, 71, 61, 67,  1, 65, 54, 47, 64, 51,  1, 55, 66,  1,\n        69, 55, 66, 54,  1, 59, 51,  1, 47, 60, 50,  1, 65, 51, 69,  1, 59, 71,\n         1, 65, 54, 55, 64, 66, 20,  3,  1, 28, 51, 64,  1, 59, 61, 59,  1, 65,\n        59, 55, 58, 51, 50,  1, 47, 60, 50,  1, 65, 47, 55, 50,  6,  1,  3, 45,\n        51, 65,  6,  1, 32, 55, 58, 71,  6,  1, 69, 51,  1, 49, 47, 60,  1, 65,\n        54, 47, 64, 51,  1, 66, 54, 51,  1, 60, 51, 51, 50, 58, 51,  1, 47, 60,\n        50,  1, 52, 55, 70,  1, 71, 61, 67, 64,  1, 65, 54, 55, 64, 66,  8,  3,\n         0,  0, 40, 61, 53, 51, 66, 54, 51, 64,  6,  1, 66, 54, 51, 71,  1, 65,\n        54, 47, 64, 51, 50,  1, 66, 54, 51,  1, 60, 51, 51, 50, 58, 51,  1, 47,\n        60, 50,  1, 65, 51, 69, 51, 50,  1, 66, 54, 51,  1, 48, 67, 66, 66, 61,\n        60,  1, 61, 60,  1, 32, 55, 58, 71,  5, 65,  1, 65, 54, 55, 64, 66,  8,\n         1, 29, 66,  1, 69, 47, 65,  1, 60, 61, 66,  1, 50, 55, 52, 52, 55, 49,\n        67, 58, 66,  1, 52, 61, 64,  1, 66, 54, 51, 59,  1, 48, 51, 49, 47, 67,\n        65, 51,  1, 66, 54, 51, 71,  1, 69, 51, 64, 51,  1, 65, 54, 47, 64, 55,\n        60, 53,  1, 47, 60, 50,  1, 54, 51, 58, 62, 55, 60, 53,  1, 51, 47, 49,\n        54,  1, 61, 66, 54, 51, 64,  8,  1, 21, 52, 66, 51, 64,  1, 66, 54, 51,\n        71,  1, 52, 55, 60, 55, 65, 54, 51, 50,  6,  1, 32, 55, 58, 71,  1, 66,\n        54, 47, 60, 57, 51, 50,  1, 54, 51, 64,  1, 59, 61, 59,  1, 52, 61, 64,\n         1, 65, 54, 47, 64, 55, 60, 53,  1, 66, 54, 51,  1, 60, 51, 51, 50, 58,\n        51,  1, 47, 60, 50,  1, 52, 55, 70, 55, 60, 53,  1, 54, 51, 64,  1, 65,\n        54, 55, 64, 66,  8,  1, 40, 54, 51, 71,  1, 48, 61, 66, 54,  1, 52, 51,\n        58, 66,  1, 54, 47, 62, 62, 71,  1, 48, 51, 49, 47, 67, 65, 51,  1, 66,\n        54, 51, 71,  1, 54, 47, 50,  1, 65, 54, 47, 64, 51, 50,  1, 47, 60, 50,\n         1, 69, 61, 64, 57, 51, 50,  1, 66, 61, 53, 51, 66, 54, 51, 64,  8,  0,\n        35, 60, 49, 51,  1, 67, 62, 61, 60,  1, 47,  1, 66, 55, 59, 51,  6,  1,\n        66, 54, 51, 64, 51,  1, 69, 47, 65,  1, 47,  1, 58, 55, 66, 66, 58, 51,\n         1, 49, 47, 64,  1, 60, 47, 59, 51, 50,  1, 22, 51, 51, 62,  8,  1, 22,\n        51, 51, 62,  1, 58, 61, 68, 51, 50,  1, 66, 61,  1, 53, 61,  1, 52, 47,\n        65, 66,  1, 47, 60, 50,  1, 62, 58, 47, 71,  1, 55, 60,  1, 66, 54, 51,\n         1, 65, 67, 60,  8,  1, 22, 51, 51, 62,  1, 69, 47, 65,  1, 47,  1, 54,\n        51, 47, 58, 66, 54, 71,  1, 49, 47, 64,  1, 48, 51, 49, 47, 67, 65, 51,\n         1, 54, 51,  1, 47, 58, 69, 47, 71, 65,  1, 54, 47, 50,  1, 53, 61, 61,\n        50,  1, 52, 67, 51, 58,  8,  1, 27, 61, 61, 50,  1, 52, 67, 51, 58,  1,\n        59, 47, 50, 51,  1, 22, 51, 51, 62,  1, 54, 47, 62, 62, 71,  1, 47, 60,\n        50,  1, 65, 66, 64, 61, 60, 53,  8,  0,  0, 35, 60, 51,  1, 50, 47, 71,\n         6,  1, 22, 51, 51, 62,  1, 69, 47, 65,  1, 50, 64, 55, 68, 55, 60, 53,\n         1, 55, 60,  1, 66, 54, 51,  1, 62, 47, 64, 57,  1, 69, 54, 51, 60,  1,\n        54, 51,  1, 65, 47, 69,  1, 47,  1, 48, 55, 53,  1, 66, 64, 51, 51,  8,\n         1, 40, 54, 51,  1, 66, 64, 51, 51,  1, 54, 47, 50,  1, 59, 47, 60, 71,\n         1, 58, 51, 47, 68, 51, 65,  1, 66, 54, 47, 66,  1, 69, 51, 64, 51,  1,\n        52, 47, 58, 58, 55, 60, 53,  8,  1, 22])\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# we will just want to split this limited data into train and validation\nn = int(0.9*len(data)) # 90% training\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:24:42.684757Z","iopub.execute_input":"2025-05-13T17:24:42.684985Z","iopub.status.idle":"2025-05-13T17:24:42.688657Z","shell.execute_reply.started":"2025-05-13T17:24:42.684968Z","shell.execute_reply":"2025-05-13T17:24:42.688035Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:20:20.586914Z","iopub.execute_input":"2025-05-13T18:20:20.587512Z","iopub.status.idle":"2025-05-13T18:20:20.591579Z","shell.execute_reply.started":"2025-05-13T18:20:20.587489Z","shell.execute_reply":"2025-05-13T18:20:20.591056Z"}},"outputs":[{"execution_count":107,"output_type":"execute_result","data":{"text/plain":"1729170"},"metadata":{}}],"execution_count":107},{"cell_type":"markdown","source":"## Defining the length of the sampled blocks","metadata":{}},{"cell_type":"code","source":"block_size = 8 # so our \"context window\" length is 8","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:24:42.689419Z","iopub.execute_input":"2025-05-13T17:24:42.689658Z","iopub.status.idle":"2025-05-13T17:24:42.703904Z","shell.execute_reply.started":"2025-05-13T17:24:42.689637Z","shell.execute_reply":"2025-05-13T17:24:42.703178Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"torch.manual_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:24:42.704619Z","iopub.execute_input":"2025-05-13T17:24:42.704983Z","iopub.status.idle":"2025-05-13T17:24:42.725440Z","shell.execute_reply.started":"2025-05-13T17:24:42.704956Z","shell.execute_reply":"2025-05-13T17:24:42.724845Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7f2f8e2fb9b0>"},"metadata":{}}],"execution_count":41},{"cell_type":"markdown","source":"## Defining the Dataset","metadata":{}},{"cell_type":"code","source":"class CharDataset(Dataset):\n    def __init__(self, data, block_size):\n        self.data = data\n        self.block_size = block_size\n\n    def __len__(self):\n        return len(self.data) - self.block_size\n\n    def __getitem__(self, idx):\n        chunk = self.data[idx : idx + self.block_size + 1]\n        x = chunk[:-1]\n        y = chunk[1:]\n        return x, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:24:42.726085Z","iopub.execute_input":"2025-05-13T17:24:42.726280Z","iopub.status.idle":"2025-05-13T17:24:42.736073Z","shell.execute_reply.started":"2025-05-13T17:24:42.726265Z","shell.execute_reply":"2025-05-13T17:24:42.735447Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"train_dataset = CharDataset(train_data, block_size)\nval_dataset = CharDataset(val_data, block_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:24:42.736608Z","iopub.execute_input":"2025-05-13T17:24:42.736857Z","iopub.status.idle":"2025-05-13T17:24:42.751613Z","shell.execute_reply.started":"2025-05-13T17:24:42.736835Z","shell.execute_reply":"2025-05-13T17:24:42.750988Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"batch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:24:42.752350Z","iopub.execute_input":"2025-05-13T17:24:42.752532Z","iopub.status.idle":"2025-05-13T17:24:42.766305Z","shell.execute_reply.started":"2025-05-13T17:24:42.752518Z","shell.execute_reply":"2025-05-13T17:24:42.765782Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"xb, yb = next(iter(train_loader))\nprint(\"inputs:\")\nprint(xb.shape)\nprint(xb)\nprint(\"targets:\")\nprint(yb.shape)\nprint(yb)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:24:42.766907Z","iopub.execute_input":"2025-05-13T17:24:42.767061Z","iopub.status.idle":"2025-05-13T17:24:42.920314Z","shell.execute_reply.started":"2025-05-13T17:24:42.767048Z","shell.execute_reply":"2025-05-13T17:24:42.919703Z"}},"outputs":[{"name":"stdout","text":"inputs:\ntorch.Size([32, 8])\ntensor([[ 1, 66, 54, 51,  1, 51, 60, 50],\n        [48, 47, 49, 57,  1, 66, 61,  1],\n        [ 1, 28, 51,  1, 49, 61, 67, 58],\n        [51,  1, 66, 54, 51,  1, 51, 47],\n        [64, 55, 50, 51,  1, 66, 54, 51],\n        [54, 61, 67, 53, 54, 66,  1, 61],\n        [ 8,  1, 28, 51,  1, 65, 47, 55],\n        [24, 47, 50,  6,  1, 82, 87, 83],\n        [60, 50,  8,  1, 24, 61, 53,  1],\n        [51, 60,  1, 69, 55, 66, 54,  1],\n        [47, 50,  8,  1, 28, 51,  1, 65],\n        [47,  1, 49, 64, 55, 51, 50,  8],\n        [61, 69,  1, 65, 69, 51, 51, 66],\n        [ 3,  1,  0,  0, 40, 55, 59, 59],\n        [55, 60, 53,  1, 66, 54, 51,  1],\n        [52, 67, 58,  1, 52, 55, 65, 54],\n        [ 1, 54, 51, 58, 62,  1, 71, 61],\n        [47, 64, 51,  1, 48, 61, 66, 54],\n        [55, 58, 71,  1, 66, 61, 58, 50],\n        [51,  1, 49, 61, 59, 62, 58, 51],\n        [64, 64, 71,  8,  1, 39, 54, 51],\n        [ 1, 48, 55, 64, 50, 65,  1, 47],\n        [55, 50,  8,  0,  0, 40, 54, 51],\n        [54, 55, 66, 51,  1, 60, 47, 62],\n        [ 1, 66, 55, 59, 51,  6,  1, 66],\n        [65,  1, 48, 58, 67, 51,  1, 47],\n        [64,  1, 47, 60, 50,  1, 49, 61],\n        [58,  1, 68, 51, 64, 71,  1, 65],\n        [60, 50,  1, 66, 54, 51,  1, 64],\n        [ 1, 59, 51, 66, 47, 58,  1, 47],\n        [51, 58, 49, 61, 59, 51,  1, 47],\n        [ 1, 48, 51, 65, 66,  1, 50, 47]])\ntargets:\ntorch.Size([32, 8])\ntensor([[66, 54, 51,  1, 51, 60, 50,  1],\n        [47, 49, 57,  1, 66, 61,  1, 66],\n        [28, 51,  1, 49, 61, 67, 58, 50],\n        [ 1, 66, 54, 51,  1, 51, 47, 53],\n        [55, 50, 51,  1, 66, 54, 51,  1],\n        [61, 67, 53, 54, 66,  1, 61, 52],\n        [ 1, 28, 51,  1, 65, 47, 55, 50],\n        [47, 50,  6,  1, 82, 87, 83, 23],\n        [50,  8,  1, 24, 61, 53,  1, 47],\n        [60,  1, 69, 55, 66, 54,  1, 47],\n        [50,  8,  1, 28, 51,  1, 65, 55],\n        [ 1, 49, 64, 55, 51, 50,  8,  1],\n        [69,  1, 65, 69, 51, 51, 66, 58],\n        [ 1,  0,  0, 40, 55, 59, 59, 71],\n        [60, 53,  1, 66, 54, 51,  1, 59],\n        [67, 58,  1, 52, 55, 65, 54,  2],\n        [54, 51, 58, 62,  1, 71, 61, 67],\n        [64, 51,  1, 48, 61, 66, 54,  1],\n        [58, 71,  1, 66, 61, 58, 50,  1],\n        [ 1, 49, 61, 59, 62, 58, 51, 66],\n        [64, 71,  8,  1, 39, 54, 51,  1],\n        [48, 55, 64, 50, 65,  1, 47, 60],\n        [50,  8,  0,  0, 40, 54, 51, 71],\n        [55, 66, 51,  1, 60, 47, 62, 57],\n        [66, 55, 59, 51,  6,  1, 66, 54],\n        [ 1, 48, 58, 67, 51,  1, 47, 60],\n        [ 1, 47, 60, 50,  1, 49, 61, 68],\n        [ 1, 68, 51, 64, 71,  1, 65, 58],\n        [50,  1, 66, 54, 51,  1, 64, 55],\n        [59, 51, 66, 47, 58,  1, 47, 60],\n        [58, 49, 61, 59, 51,  1, 47,  1],\n        [48, 51, 65, 66,  1, 50, 47, 71]])\n","output_type":"stream"}],"execution_count":45},{"cell_type":"markdown","source":"## Context-target preview","metadata":{}},{"cell_type":"code","source":"for b in range(batch_size):\n    for t in range(block_size):\n        context = xb[b, :t+1]\n        target = yb[b, t]\n        print(f\"when input is {context.tolist()} the target: {target}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:24:42.921044Z","iopub.execute_input":"2025-05-13T17:24:42.921320Z","iopub.status.idle":"2025-05-13T17:24:42.930825Z","shell.execute_reply.started":"2025-05-13T17:24:42.921295Z","shell.execute_reply":"2025-05-13T17:24:42.930117Z"}},"outputs":[{"name":"stdout","text":"when input is [1] the target: 66\nwhen input is [1, 66] the target: 54\nwhen input is [1, 66, 54] the target: 51\nwhen input is [1, 66, 54, 51] the target: 1\nwhen input is [1, 66, 54, 51, 1] the target: 51\nwhen input is [1, 66, 54, 51, 1, 51] the target: 60\nwhen input is [1, 66, 54, 51, 1, 51, 60] the target: 50\nwhen input is [1, 66, 54, 51, 1, 51, 60, 50] the target: 1\nwhen input is [48] the target: 47\nwhen input is [48, 47] the target: 49\nwhen input is [48, 47, 49] the target: 57\nwhen input is [48, 47, 49, 57] the target: 1\nwhen input is [48, 47, 49, 57, 1] the target: 66\nwhen input is [48, 47, 49, 57, 1, 66] the target: 61\nwhen input is [48, 47, 49, 57, 1, 66, 61] the target: 1\nwhen input is [48, 47, 49, 57, 1, 66, 61, 1] the target: 66\nwhen input is [1] the target: 28\nwhen input is [1, 28] the target: 51\nwhen input is [1, 28, 51] the target: 1\nwhen input is [1, 28, 51, 1] the target: 49\nwhen input is [1, 28, 51, 1, 49] the target: 61\nwhen input is [1, 28, 51, 1, 49, 61] the target: 67\nwhen input is [1, 28, 51, 1, 49, 61, 67] the target: 58\nwhen input is [1, 28, 51, 1, 49, 61, 67, 58] the target: 50\nwhen input is [51] the target: 1\nwhen input is [51, 1] the target: 66\nwhen input is [51, 1, 66] the target: 54\nwhen input is [51, 1, 66, 54] the target: 51\nwhen input is [51, 1, 66, 54, 51] the target: 1\nwhen input is [51, 1, 66, 54, 51, 1] the target: 51\nwhen input is [51, 1, 66, 54, 51, 1, 51] the target: 47\nwhen input is [51, 1, 66, 54, 51, 1, 51, 47] the target: 53\nwhen input is [64] the target: 55\nwhen input is [64, 55] the target: 50\nwhen input is [64, 55, 50] the target: 51\nwhen input is [64, 55, 50, 51] the target: 1\nwhen input is [64, 55, 50, 51, 1] the target: 66\nwhen input is [64, 55, 50, 51, 1, 66] the target: 54\nwhen input is [64, 55, 50, 51, 1, 66, 54] the target: 51\nwhen input is [64, 55, 50, 51, 1, 66, 54, 51] the target: 1\nwhen input is [54] the target: 61\nwhen input is [54, 61] the target: 67\nwhen input is [54, 61, 67] the target: 53\nwhen input is [54, 61, 67, 53] the target: 54\nwhen input is [54, 61, 67, 53, 54] the target: 66\nwhen input is [54, 61, 67, 53, 54, 66] the target: 1\nwhen input is [54, 61, 67, 53, 54, 66, 1] the target: 61\nwhen input is [54, 61, 67, 53, 54, 66, 1, 61] the target: 52\nwhen input is [8] the target: 1\nwhen input is [8, 1] the target: 28\nwhen input is [8, 1, 28] the target: 51\nwhen input is [8, 1, 28, 51] the target: 1\nwhen input is [8, 1, 28, 51, 1] the target: 65\nwhen input is [8, 1, 28, 51, 1, 65] the target: 47\nwhen input is [8, 1, 28, 51, 1, 65, 47] the target: 55\nwhen input is [8, 1, 28, 51, 1, 65, 47, 55] the target: 50\nwhen input is [24] the target: 47\nwhen input is [24, 47] the target: 50\nwhen input is [24, 47, 50] the target: 6\nwhen input is [24, 47, 50, 6] the target: 1\nwhen input is [24, 47, 50, 6, 1] the target: 82\nwhen input is [24, 47, 50, 6, 1, 82] the target: 87\nwhen input is [24, 47, 50, 6, 1, 82, 87] the target: 83\nwhen input is [24, 47, 50, 6, 1, 82, 87, 83] the target: 23\nwhen input is [60] the target: 50\nwhen input is [60, 50] the target: 8\nwhen input is [60, 50, 8] the target: 1\nwhen input is [60, 50, 8, 1] the target: 24\nwhen input is [60, 50, 8, 1, 24] the target: 61\nwhen input is [60, 50, 8, 1, 24, 61] the target: 53\nwhen input is [60, 50, 8, 1, 24, 61, 53] the target: 1\nwhen input is [60, 50, 8, 1, 24, 61, 53, 1] the target: 47\nwhen input is [51] the target: 60\nwhen input is [51, 60] the target: 1\nwhen input is [51, 60, 1] the target: 69\nwhen input is [51, 60, 1, 69] the target: 55\nwhen input is [51, 60, 1, 69, 55] the target: 66\nwhen input is [51, 60, 1, 69, 55, 66] the target: 54\nwhen input is [51, 60, 1, 69, 55, 66, 54] the target: 1\nwhen input is [51, 60, 1, 69, 55, 66, 54, 1] the target: 47\nwhen input is [47] the target: 50\nwhen input is [47, 50] the target: 8\nwhen input is [47, 50, 8] the target: 1\nwhen input is [47, 50, 8, 1] the target: 28\nwhen input is [47, 50, 8, 1, 28] the target: 51\nwhen input is [47, 50, 8, 1, 28, 51] the target: 1\nwhen input is [47, 50, 8, 1, 28, 51, 1] the target: 65\nwhen input is [47, 50, 8, 1, 28, 51, 1, 65] the target: 55\nwhen input is [47] the target: 1\nwhen input is [47, 1] the target: 49\nwhen input is [47, 1, 49] the target: 64\nwhen input is [47, 1, 49, 64] the target: 55\nwhen input is [47, 1, 49, 64, 55] the target: 51\nwhen input is [47, 1, 49, 64, 55, 51] the target: 50\nwhen input is [47, 1, 49, 64, 55, 51, 50] the target: 8\nwhen input is [47, 1, 49, 64, 55, 51, 50, 8] the target: 1\nwhen input is [61] the target: 69\nwhen input is [61, 69] the target: 1\nwhen input is [61, 69, 1] the target: 65\nwhen input is [61, 69, 1, 65] the target: 69\nwhen input is [61, 69, 1, 65, 69] the target: 51\nwhen input is [61, 69, 1, 65, 69, 51] the target: 51\nwhen input is [61, 69, 1, 65, 69, 51, 51] the target: 66\nwhen input is [61, 69, 1, 65, 69, 51, 51, 66] the target: 58\nwhen input is [3] the target: 1\nwhen input is [3, 1] the target: 0\nwhen input is [3, 1, 0] the target: 0\nwhen input is [3, 1, 0, 0] the target: 40\nwhen input is [3, 1, 0, 0, 40] the target: 55\nwhen input is [3, 1, 0, 0, 40, 55] the target: 59\nwhen input is [3, 1, 0, 0, 40, 55, 59] the target: 59\nwhen input is [3, 1, 0, 0, 40, 55, 59, 59] the target: 71\nwhen input is [55] the target: 60\nwhen input is [55, 60] the target: 53\nwhen input is [55, 60, 53] the target: 1\nwhen input is [55, 60, 53, 1] the target: 66\nwhen input is [55, 60, 53, 1, 66] the target: 54\nwhen input is [55, 60, 53, 1, 66, 54] the target: 51\nwhen input is [55, 60, 53, 1, 66, 54, 51] the target: 1\nwhen input is [55, 60, 53, 1, 66, 54, 51, 1] the target: 59\nwhen input is [52] the target: 67\nwhen input is [52, 67] the target: 58\nwhen input is [52, 67, 58] the target: 1\nwhen input is [52, 67, 58, 1] the target: 52\nwhen input is [52, 67, 58, 1, 52] the target: 55\nwhen input is [52, 67, 58, 1, 52, 55] the target: 65\nwhen input is [52, 67, 58, 1, 52, 55, 65] the target: 54\nwhen input is [52, 67, 58, 1, 52, 55, 65, 54] the target: 2\nwhen input is [1] the target: 54\nwhen input is [1, 54] the target: 51\nwhen input is [1, 54, 51] the target: 58\nwhen input is [1, 54, 51, 58] the target: 62\nwhen input is [1, 54, 51, 58, 62] the target: 1\nwhen input is [1, 54, 51, 58, 62, 1] the target: 71\nwhen input is [1, 54, 51, 58, 62, 1, 71] the target: 61\nwhen input is [1, 54, 51, 58, 62, 1, 71, 61] the target: 67\nwhen input is [47] the target: 64\nwhen input is [47, 64] the target: 51\nwhen input is [47, 64, 51] the target: 1\nwhen input is [47, 64, 51, 1] the target: 48\nwhen input is [47, 64, 51, 1, 48] the target: 61\nwhen input is [47, 64, 51, 1, 48, 61] the target: 66\nwhen input is [47, 64, 51, 1, 48, 61, 66] the target: 54\nwhen input is [47, 64, 51, 1, 48, 61, 66, 54] the target: 1\nwhen input is [55] the target: 58\nwhen input is [55, 58] the target: 71\nwhen input is [55, 58, 71] the target: 1\nwhen input is [55, 58, 71, 1] the target: 66\nwhen input is [55, 58, 71, 1, 66] the target: 61\nwhen input is [55, 58, 71, 1, 66, 61] the target: 58\nwhen input is [55, 58, 71, 1, 66, 61, 58] the target: 50\nwhen input is [55, 58, 71, 1, 66, 61, 58, 50] the target: 1\nwhen input is [51] the target: 1\nwhen input is [51, 1] the target: 49\nwhen input is [51, 1, 49] the target: 61\nwhen input is [51, 1, 49, 61] the target: 59\nwhen input is [51, 1, 49, 61, 59] the target: 62\nwhen input is [51, 1, 49, 61, 59, 62] the target: 58\nwhen input is [51, 1, 49, 61, 59, 62, 58] the target: 51\nwhen input is [51, 1, 49, 61, 59, 62, 58, 51] the target: 66\nwhen input is [64] the target: 64\nwhen input is [64, 64] the target: 71\nwhen input is [64, 64, 71] the target: 8\nwhen input is [64, 64, 71, 8] the target: 1\nwhen input is [64, 64, 71, 8, 1] the target: 39\nwhen input is [64, 64, 71, 8, 1, 39] the target: 54\nwhen input is [64, 64, 71, 8, 1, 39, 54] the target: 51\nwhen input is [64, 64, 71, 8, 1, 39, 54, 51] the target: 1\nwhen input is [1] the target: 48\nwhen input is [1, 48] the target: 55\nwhen input is [1, 48, 55] the target: 64\nwhen input is [1, 48, 55, 64] the target: 50\nwhen input is [1, 48, 55, 64, 50] the target: 65\nwhen input is [1, 48, 55, 64, 50, 65] the target: 1\nwhen input is [1, 48, 55, 64, 50, 65, 1] the target: 47\nwhen input is [1, 48, 55, 64, 50, 65, 1, 47] the target: 60\nwhen input is [55] the target: 50\nwhen input is [55, 50] the target: 8\nwhen input is [55, 50, 8] the target: 0\nwhen input is [55, 50, 8, 0] the target: 0\nwhen input is [55, 50, 8, 0, 0] the target: 40\nwhen input is [55, 50, 8, 0, 0, 40] the target: 54\nwhen input is [55, 50, 8, 0, 0, 40, 54] the target: 51\nwhen input is [55, 50, 8, 0, 0, 40, 54, 51] the target: 71\nwhen input is [54] the target: 55\nwhen input is [54, 55] the target: 66\nwhen input is [54, 55, 66] the target: 51\nwhen input is [54, 55, 66, 51] the target: 1\nwhen input is [54, 55, 66, 51, 1] the target: 60\nwhen input is [54, 55, 66, 51, 1, 60] the target: 47\nwhen input is [54, 55, 66, 51, 1, 60, 47] the target: 62\nwhen input is [54, 55, 66, 51, 1, 60, 47, 62] the target: 57\nwhen input is [1] the target: 66\nwhen input is [1, 66] the target: 55\nwhen input is [1, 66, 55] the target: 59\nwhen input is [1, 66, 55, 59] the target: 51\nwhen input is [1, 66, 55, 59, 51] the target: 6\nwhen input is [1, 66, 55, 59, 51, 6] the target: 1\nwhen input is [1, 66, 55, 59, 51, 6, 1] the target: 66\nwhen input is [1, 66, 55, 59, 51, 6, 1, 66] the target: 54\nwhen input is [65] the target: 1\nwhen input is [65, 1] the target: 48\nwhen input is [65, 1, 48] the target: 58\nwhen input is [65, 1, 48, 58] the target: 67\nwhen input is [65, 1, 48, 58, 67] the target: 51\nwhen input is [65, 1, 48, 58, 67, 51] the target: 1\nwhen input is [65, 1, 48, 58, 67, 51, 1] the target: 47\nwhen input is [65, 1, 48, 58, 67, 51, 1, 47] the target: 60\nwhen input is [64] the target: 1\nwhen input is [64, 1] the target: 47\nwhen input is [64, 1, 47] the target: 60\nwhen input is [64, 1, 47, 60] the target: 50\nwhen input is [64, 1, 47, 60, 50] the target: 1\nwhen input is [64, 1, 47, 60, 50, 1] the target: 49\nwhen input is [64, 1, 47, 60, 50, 1, 49] the target: 61\nwhen input is [64, 1, 47, 60, 50, 1, 49, 61] the target: 68\nwhen input is [58] the target: 1\nwhen input is [58, 1] the target: 68\nwhen input is [58, 1, 68] the target: 51\nwhen input is [58, 1, 68, 51] the target: 64\nwhen input is [58, 1, 68, 51, 64] the target: 71\nwhen input is [58, 1, 68, 51, 64, 71] the target: 1\nwhen input is [58, 1, 68, 51, 64, 71, 1] the target: 65\nwhen input is [58, 1, 68, 51, 64, 71, 1, 65] the target: 58\nwhen input is [60] the target: 50\nwhen input is [60, 50] the target: 1\nwhen input is [60, 50, 1] the target: 66\nwhen input is [60, 50, 1, 66] the target: 54\nwhen input is [60, 50, 1, 66, 54] the target: 51\nwhen input is [60, 50, 1, 66, 54, 51] the target: 1\nwhen input is [60, 50, 1, 66, 54, 51, 1] the target: 64\nwhen input is [60, 50, 1, 66, 54, 51, 1, 64] the target: 55\nwhen input is [1] the target: 59\nwhen input is [1, 59] the target: 51\nwhen input is [1, 59, 51] the target: 66\nwhen input is [1, 59, 51, 66] the target: 47\nwhen input is [1, 59, 51, 66, 47] the target: 58\nwhen input is [1, 59, 51, 66, 47, 58] the target: 1\nwhen input is [1, 59, 51, 66, 47, 58, 1] the target: 47\nwhen input is [1, 59, 51, 66, 47, 58, 1, 47] the target: 60\nwhen input is [51] the target: 58\nwhen input is [51, 58] the target: 49\nwhen input is [51, 58, 49] the target: 61\nwhen input is [51, 58, 49, 61] the target: 59\nwhen input is [51, 58, 49, 61, 59] the target: 51\nwhen input is [51, 58, 49, 61, 59, 51] the target: 1\nwhen input is [51, 58, 49, 61, 59, 51, 1] the target: 47\nwhen input is [51, 58, 49, 61, 59, 51, 1, 47] the target: 1\nwhen input is [1] the target: 48\nwhen input is [1, 48] the target: 51\nwhen input is [1, 48, 51] the target: 65\nwhen input is [1, 48, 51, 65] the target: 66\nwhen input is [1, 48, 51, 65, 66] the target: 1\nwhen input is [1, 48, 51, 65, 66, 1] the target: 50\nwhen input is [1, 48, 51, 65, 66, 1, 50] the target: 47\nwhen input is [1, 48, 51, 65, 66, 1, 50, 47] the target: 71\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"## Bigram Language Model\n* The vocabulary size is vocab_size.\n* The model uses an embedding table of shape (vocab_size, vocab_size) to directly map each input token to the logits over the next token.","metadata":{}},{"cell_type":"code","source":"class BiGramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # embedding layer (it's like a lookup table) that maps each token (index) to a vector of size vocab_size.\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # B: batch size\n        # T: time/context length\n        # idx.shape = (B, T) --> used as a query to the lookup table\n        # targets.shape = (B, T) --> each idx is mapped to one fixed ground truth target\n        logits = self.token_embedding_table(idx) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, vocab_size = logits.shape\n            # logits: for each token position in the batch, the model outputs a probability distribution over all possible next tokens.\n            # targets: the correct next token index for each position.\n            logits = logits.view(B*T, vocab_size)\n            targets = targets.view(B*T)\n            # We use cross-entropy loss to measure how well the predicted probability distribution (from logits) matches the true distribution (one-hot at the correct target index).\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # only the logits from the last time step are required as we want to find the logits for the next time step based on the current time step\n            # since it’s a bigram model, it has no context window, only 1-step memory, so the output is essentially a Markov chain\n            logits = logits[:, -1, :] # becomes (B, vocab_size)\n            # softmaxxing for probabilities\n            probs = F.softmax(logits, dim=-1) # (B, vocab_size)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:24:42.931807Z","iopub.execute_input":"2025-05-13T17:24:42.932088Z","iopub.status.idle":"2025-05-13T17:24:42.942743Z","shell.execute_reply.started":"2025-05-13T17:24:42.932066Z","shell.execute_reply":"2025-05-13T17:24:42.942076Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:32:24.730616Z","iopub.execute_input":"2025-05-13T17:32:24.731392Z","iopub.status.idle":"2025-05-13T17:32:24.734884Z","shell.execute_reply.started":"2025-05-13T17:32:24.731368Z","shell.execute_reply":"2025-05-13T17:32:24.734080Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"bgm = BiGramLanguageModel(vocab_size).to(device)\nlogits, loss = bgm(xb, yb)\nprint(logits.shape)\nprint(loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:32:16.793206Z","iopub.execute_input":"2025-05-13T17:32:16.793483Z","iopub.status.idle":"2025-05-13T17:32:16.800303Z","shell.execute_reply.started":"2025-05-13T17:32:16.793462Z","shell.execute_reply":"2025-05-13T17:32:16.799713Z"}},"outputs":[{"name":"stdout","text":"torch.Size([256, 89])\ntensor(4.9285, device='cuda:0', grad_fn=<NllLossBackward0>)\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"print(decode(bgm.generate(idx = torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=100)[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:35:01.662414Z","iopub.execute_input":"2025-05-13T17:35:01.663081Z","iopub.status.idle":"2025-05-13T17:35:01.862791Z","shell.execute_reply.started":"2025-05-13T17:35:01.663055Z","shell.execute_reply":"2025-05-13T17:35:01.862013Z"}},"outputs":[{"name":"stdout","text":"\n¦sœ? xW2XBâx!7H'pfL©!:©wuQ:ztVPJS2©˜7±ZKHQTpU¦B,œs?WRâuHPxA--4œ4pCDJE“o6Â6âEA”ALH³€g±YgH,Gy4My¦x.O;»\n","output_type":"stream"}],"execution_count":61},{"cell_type":"markdown","source":"## Training BiGramLanguageModel","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(bgm.parameters(), lr=1e-3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:37:02.011097Z","iopub.execute_input":"2025-05-13T17:37:02.011770Z","iopub.status.idle":"2025-05-13T17:37:02.015277Z","shell.execute_reply.started":"2025-05-13T17:37:02.011744Z","shell.execute_reply":"2025-05-13T17:37:02.014556Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"train_losses, val_losses = [], []\ntrain_accuracies, val_accuracies = [], []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:37:02.156139Z","iopub.execute_input":"2025-05-13T17:37:02.156404Z","iopub.status.idle":"2025-05-13T17:37:02.159822Z","shell.execute_reply.started":"2025-05-13T17:37:02.156385Z","shell.execute_reply":"2025-05-13T17:37:02.159144Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"batch_size = 32\nblock_size = 8\neval_interval = 500\nepochs = 20000\neval_iters = 200","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:37:02.305581Z","iopub.execute_input":"2025-05-13T17:37:02.306067Z","iopub.status.idle":"2025-05-13T17:37:02.309435Z","shell.execute_reply.started":"2025-05-13T17:37:02.306040Z","shell.execute_reply":"2025-05-13T17:37:02.308813Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"def compute_accuracy(logits, targets):\n    preds = torch.argmax(logits, dim=-1)\n    correct = (preds == targets).float()\n    return correct.sum() / correct.numel()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:37:04.111908Z","iopub.execute_input":"2025-05-13T17:37:04.112398Z","iopub.status.idle":"2025-05-13T17:37:04.116449Z","shell.execute_reply.started":"2025-05-13T17:37:04.112374Z","shell.execute_reply":"2025-05-13T17:37:04.115685Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:37:04.272751Z","iopub.execute_input":"2025-05-13T17:37:04.273298Z","iopub.status.idle":"2025-05-13T17:37:04.277698Z","shell.execute_reply.started":"2025-05-13T17:37:04.273276Z","shell.execute_reply":"2025-05-13T17:37:04.276837Z"}},"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"step = 0\nbgm.train()\npbar = tqdm(total=epochs)\ntrain_iter = iter(train_loader)\n\nwhile step < epochs:\n    try:\n        xb, yb = next(train_iter)\n    except StopIteration:\n        train_iter = iter(train_loader)\n        xb, yb = next(train_iter)\n\n    xb, yb = xb.to(device), yb.to(device)\n    # print(xb.shape, yb.shape)\n    logits, loss = bgm(xb, yb)\n    if(step%100 == 0):\n        print(f\"Epoch[{step+1}/{epochs}] ---- Training loss = {loss}\")\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # record training metrics every eval_interval steps\n    if step % eval_interval == 0:\n        bgm.eval()\n        with torch.no_grad():\n            # training eval\n            train_logits, _ = bgm(xb, yb)\n            train_acc = compute_accuracy(train_logits.view(-1, vocab_size), yb.view(-1)).item()\n            train_losses.append(loss.item())\n            train_accuracies.append(train_acc)\n\n            #  validation eval\n            val_loss_total, val_correct, val_total = 0.0, 0, 0\n            for val_xb, val_yb in val_loader:\n                val_xb, val_yb = val_xb.to(device), val_yb.to(device)\n                val_logits, val_loss = bgm(val_xb, val_yb)\n                val_loss_total += val_loss.item()\n                \n                preds = val_logits.view(-1, vocab_size).argmax(dim=-1)  # shape: (B*T,)\n                targets = val_yb.view(-1)  # shape: (B*T,)\n                val_correct += (preds == targets).sum().item()\n                val_total += val_yb.numel()\n            \n            val_losses.append(val_loss_total / len(val_loader))\n            val_accuracies.append(val_correct / val_total)\n            print(f\"Epoch[{step+1}/{epochs}] ---- VALIDATION LOSS = {val_loss_total} --- VALIDATION ACC = {val_correct/val_total}\")\n        bgm.train()\n    \n    step += 1\n    pbar.update(1)\n\npbar.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:37:04.975606Z","iopub.execute_input":"2025-05-13T17:37:04.976298Z","iopub.status.idle":"2025-05-13T17:39:45.726926Z","shell.execute_reply.started":"2025-05-13T17:37:04.976274Z","shell.execute_reply":"2025-05-13T17:39:45.726122Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21df3548ba4a4096b90228a25bfa55ad"}},"metadata":{}},{"name":"stdout","text":"Epoch[1/20000] ---- Training loss = 4.922492504119873\nEpoch[1/20000] ---- VALIDATION LOSS = 29878.781811237335 --- VALIDATION ACC = 0.0012440012075660258\nEpoch[101/20000] ---- Training loss = 4.875812530517578\nEpoch[201/20000] ---- Training loss = 4.686099529266357\nEpoch[301/20000] ---- Training loss = 4.595996379852295\nEpoch[401/20000] ---- Training loss = 4.52496337890625\nEpoch[501/20000] ---- Training loss = 4.364192485809326\nEpoch[501/20000] ---- VALIDATION LOSS = 26125.220281362534 --- VALIDATION ACC = 0.01293058577362301\nEpoch[601/20000] ---- Training loss = 4.305454254150391\nEpoch[701/20000] ---- Training loss = 4.176881790161133\nEpoch[801/20000] ---- Training loss = 4.035976886749268\nEpoch[901/20000] ---- Training loss = 4.035033702850342\nEpoch[1001/20000] ---- Training loss = 3.858185291290283\nEpoch[1001/20000] ---- VALIDATION LOSS = 23004.388361930847 --- VALIDATION ACC = 0.1464896003581058\nEpoch[1101/20000] ---- Training loss = 3.826618194580078\nEpoch[1201/20000] ---- Training loss = 3.63401198387146\nEpoch[1301/20000] ---- Training loss = 3.619093894958496\nEpoch[1401/20000] ---- Training loss = 3.472841739654541\nEpoch[1501/20000] ---- Training loss = 3.3942995071411133\nEpoch[1501/20000] ---- VALIDATION LOSS = 20531.06333732605 --- VALIDATION ACC = 0.20917893317787656\nEpoch[1601/20000] ---- Training loss = 3.307206630706787\nEpoch[1701/20000] ---- Training loss = 3.279853105545044\nEpoch[1801/20000] ---- Training loss = 3.196542501449585\nEpoch[1901/20000] ---- Training loss = 3.21991229057312\nEpoch[2001/20000] ---- Training loss = 3.030198574066162\nEpoch[2001/20000] ---- VALIDATION LOSS = 18664.693786621094 --- VALIDATION ACC = 0.21895397195532007\nEpoch[2101/20000] ---- Training loss = 2.9695143699645996\nEpoch[2201/20000] ---- Training loss = 2.904585361480713\nEpoch[2301/20000] ---- Training loss = 2.9891533851623535\nEpoch[2401/20000] ---- Training loss = 2.941720485687256\nEpoch[2501/20000] ---- Training loss = 2.877953052520752\nEpoch[2501/20000] ---- VALIDATION LOSS = 17296.7198240757 --- VALIDATION ACC = 0.2456869853530569\nEpoch[2601/20000] ---- Training loss = 2.752410411834717\nEpoch[2701/20000] ---- Training loss = 2.74298095703125\nEpoch[2801/20000] ---- Training loss = 2.7778892517089844\nEpoch[2901/20000] ---- Training loss = 2.7732062339782715\nEpoch[3001/20000] ---- Training loss = 2.645956039428711\nEpoch[3001/20000] ---- VALIDATION LOSS = 16326.379889011383 --- VALIDATION ACC = 0.30177309209772957\nEpoch[3101/20000] ---- Training loss = 2.6255922317504883\nEpoch[3201/20000] ---- Training loss = 2.6648478507995605\nEpoch[3301/20000] ---- Training loss = 2.7870748043060303\nEpoch[3401/20000] ---- Training loss = 2.5565061569213867\nEpoch[3501/20000] ---- Training loss = 2.727001667022705\nEpoch[3501/20000] ---- VALIDATION LOSS = 15644.171501398087 --- VALIDATION ACC = 0.3019136277990027\nEpoch[3601/20000] ---- Training loss = 2.5146799087524414\nEpoch[3701/20000] ---- Training loss = 2.651923656463623\nEpoch[3801/20000] ---- Training loss = 2.4813833236694336\nEpoch[3901/20000] ---- Training loss = 2.62203311920166\nEpoch[4001/20000] ---- Training loss = 2.5463900566101074\nEpoch[4001/20000] ---- VALIDATION LOSS = 15172.33965909481 --- VALIDATION ACC = 0.3128630505616223\nEpoch[4101/20000] ---- Training loss = 2.463104248046875\nEpoch[4201/20000] ---- Training loss = 2.4940121173858643\nEpoch[4301/20000] ---- Training loss = 2.6391818523406982\nEpoch[4401/20000] ---- Training loss = 2.4347801208496094\nEpoch[4501/20000] ---- Training loss = 2.418911933898926\nEpoch[4501/20000] ---- VALIDATION LOSS = 14844.020748972893 --- VALIDATION ACC = 0.31528208638261107\nEpoch[4601/20000] ---- Training loss = 2.473139762878418\nEpoch[4701/20000] ---- Training loss = 2.526512861251831\nEpoch[4801/20000] ---- Training loss = 2.4587061405181885\nEpoch[4901/20000] ---- Training loss = 2.505265235900879\nEpoch[5001/20000] ---- Training loss = 2.34993314743042\nEpoch[5001/20000] ---- VALIDATION LOSS = 14607.197339177132 --- VALIDATION ACC = 0.31910778047282456\nEpoch[5101/20000] ---- Training loss = 2.4183051586151123\nEpoch[5201/20000] ---- Training loss = 2.3213300704956055\nEpoch[5301/20000] ---- Training loss = 2.4492156505584717\nEpoch[5401/20000] ---- Training loss = 2.4647490978240967\nEpoch[5501/20000] ---- Training loss = 2.413938283920288\nEpoch[5501/20000] ---- VALIDATION LOSS = 14436.388526439667 --- VALIDATION ACC = 0.3201019404336828\nEpoch[5601/20000] ---- Training loss = 2.267469644546509\nEpoch[5701/20000] ---- Training loss = 2.2848334312438965\nEpoch[5801/20000] ---- Training loss = 2.304293394088745\nEpoch[5901/20000] ---- Training loss = 2.269657611846924\nEpoch[6001/20000] ---- Training loss = 2.2950918674468994\nEpoch[6001/20000] ---- VALIDATION LOSS = 14309.243327140808 --- VALIDATION ACC = 0.32018522084925205\nEpoch[6101/20000] ---- Training loss = 2.407600164413452\nEpoch[6201/20000] ---- Training loss = 2.3038673400878906\nEpoch[6301/20000] ---- Training loss = 2.319467544555664\nEpoch[6401/20000] ---- Training loss = 2.3312764167785645\nEpoch[6501/20000] ---- Training loss = 2.1797077655792236\nEpoch[6501/20000] ---- VALIDATION LOSS = 14213.10300219059 --- VALIDATION ACC = 0.32018522084925205\nEpoch[6601/20000] ---- Training loss = 2.352569580078125\nEpoch[6701/20000] ---- Training loss = 2.4665987491607666\nEpoch[6801/20000] ---- Training loss = 2.3535821437835693\nEpoch[6901/20000] ---- Training loss = 2.273993730545044\nEpoch[7001/20000] ---- Training loss = 2.2847108840942383\nEpoch[7001/20000] ---- VALIDATION LOSS = 14136.363419890404 --- VALIDATION ACC = 0.32018522084925205\nEpoch[7101/20000] ---- Training loss = 2.398702621459961\nEpoch[7201/20000] ---- Training loss = 2.2669124603271484\nEpoch[7301/20000] ---- Training loss = 2.425349712371826\nEpoch[7401/20000] ---- Training loss = 2.4051578044891357\nEpoch[7501/20000] ---- Training loss = 2.269028425216675\nEpoch[7501/20000] ---- VALIDATION LOSS = 14073.97213613987 --- VALIDATION ACC = 0.3198312790830826\nEpoch[7601/20000] ---- Training loss = 2.3361728191375732\nEpoch[7701/20000] ---- Training loss = 2.368983507156372\nEpoch[7801/20000] ---- Training loss = 2.342902660369873\nEpoch[7901/20000] ---- Training loss = 2.224785804748535\nEpoch[8001/20000] ---- Training loss = 2.4158565998077393\nEpoch[8001/20000] ---- VALIDATION LOSS = 14025.748650193214 --- VALIDATION ACC = 0.32008112032979047\nEpoch[8101/20000] ---- Training loss = 2.359832286834717\nEpoch[8201/20000] ---- Training loss = 2.2617135047912598\nEpoch[8301/20000] ---- Training loss = 2.4380745887756348\nEpoch[8401/20000] ---- Training loss = 2.364187002182007\nEpoch[8501/20000] ---- Training loss = 2.3725154399871826\nEpoch[8501/20000] ---- VALIDATION LOSS = 13986.371796369553 --- VALIDATION ACC = 0.32008112032979047\nEpoch[8601/20000] ---- Training loss = 2.241298198699951\nEpoch[8701/20000] ---- Training loss = 2.4614791870117188\nEpoch[8801/20000] ---- Training loss = 2.510169267654419\nEpoch[8901/20000] ---- Training loss = 2.223773717880249\nEpoch[9001/20000] ---- Training loss = 2.347493886947632\nEpoch[9001/20000] ---- VALIDATION LOSS = 13956.08938229084 --- VALIDATION ACC = 0.3208514641738062\nEpoch[9101/20000] ---- Training loss = 2.2291741371154785\nEpoch[9201/20000] ---- Training loss = 2.1276373863220215\nEpoch[9301/20000] ---- Training loss = 2.2711374759674072\nEpoch[9401/20000] ---- Training loss = 2.2658843994140625\nEpoch[9501/20000] ---- Training loss = 2.383405923843384\nEpoch[9501/20000] ---- VALIDATION LOSS = 13929.783578395844 --- VALIDATION ACC = 0.32213710558915687\nEpoch[9601/20000] ---- Training loss = 2.264263153076172\nEpoch[9701/20000] ---- Training loss = 2.3417155742645264\nEpoch[9801/20000] ---- Training loss = 2.2633416652679443\nEpoch[9901/20000] ---- Training loss = 2.258164405822754\nEpoch[10001/20000] ---- Training loss = 2.221224069595337\nEpoch[10001/20000] ---- VALIDATION LOSS = 13910.112794160843 --- VALIDATION ACC = 0.32213710558915687\nEpoch[10101/20000] ---- Training loss = 2.125521183013916\nEpoch[10201/20000] ---- Training loss = 2.283991813659668\nEpoch[10301/20000] ---- Training loss = 2.2271578311920166\nEpoch[10401/20000] ---- Training loss = 2.221388101577759\nEpoch[10501/20000] ---- Training loss = 2.3873162269592285\nEpoch[10501/20000] ---- VALIDATION LOSS = 13889.003419041634 --- VALIDATION ACC = 0.3221579256930492\nEpoch[10601/20000] ---- Training loss = 2.375424385070801\nEpoch[10701/20000] ---- Training loss = 2.1334171295166016\nEpoch[10801/20000] ---- Training loss = 2.403163194656372\nEpoch[10901/20000] ---- Training loss = 2.3629322052001953\nEpoch[11001/20000] ---- Training loss = 2.413435220718384\nEpoch[11001/20000] ---- VALIDATION LOSS = 13874.574427366257 --- VALIDATION ACC = 0.3221579256930492\nEpoch[11101/20000] ---- Training loss = 2.2494688034057617\nEpoch[11201/20000] ---- Training loss = 2.222945213317871\nEpoch[11301/20000] ---- Training loss = 2.3101513385772705\nEpoch[11401/20000] ---- Training loss = 2.220740556716919\nEpoch[11501/20000] ---- Training loss = 2.353029489517212\nEpoch[11501/20000] ---- VALIDATION LOSS = 13863.252213001251 --- VALIDATION ACC = 0.3221579256930492\nEpoch[11601/20000] ---- Training loss = 2.338535785675049\nEpoch[11701/20000] ---- Training loss = 2.3782236576080322\nEpoch[11801/20000] ---- Training loss = 2.121248483657837\nEpoch[11901/20000] ---- Training loss = 2.298326015472412\nEpoch[12001/20000] ---- Training loss = 2.2496302127838135\nEpoch[12001/20000] ---- VALIDATION LOSS = 13853.867202997208 --- VALIDATION ACC = 0.3221579256930492\nEpoch[12101/20000] ---- Training loss = 2.3617143630981445\nEpoch[12201/20000] ---- Training loss = 2.3344619274139404\nEpoch[12301/20000] ---- Training loss = 2.3116912841796875\nEpoch[12401/20000] ---- Training loss = 2.247573137283325\nEpoch[12501/20000] ---- Training loss = 2.3002398014068604\nEpoch[12501/20000] ---- VALIDATION LOSS = 13845.936529874802 --- VALIDATION ACC = 0.3221579256930492\nEpoch[12601/20000] ---- Training loss = 2.3506834506988525\nEpoch[12701/20000] ---- Training loss = 2.36968994140625\nEpoch[12801/20000] ---- Training loss = 2.2495086193084717\nEpoch[12901/20000] ---- Training loss = 2.38128924369812\nEpoch[13001/20000] ---- Training loss = 2.259049415588379\nEpoch[13001/20000] ---- VALIDATION LOSS = 13839.042575359344 --- VALIDATION ACC = 0.3221110804592915\nEpoch[13101/20000] ---- Training loss = 2.3677499294281006\nEpoch[13201/20000] ---- Training loss = 2.4116785526275635\nEpoch[13301/20000] ---- Training loss = 2.2984299659729004\nEpoch[13401/20000] ---- Training loss = 2.4017763137817383\nEpoch[13501/20000] ---- Training loss = 2.267460346221924\nEpoch[13501/20000] ---- VALIDATION LOSS = 13834.055196285248 --- VALIDATION ACC = 0.3221110804592915\nEpoch[13601/20000] ---- Training loss = 2.2961971759796143\nEpoch[13701/20000] ---- Training loss = 2.2301554679870605\nEpoch[13801/20000] ---- Training loss = 2.3918910026550293\nEpoch[13901/20000] ---- Training loss = 2.260615825653076\nEpoch[14001/20000] ---- Training loss = 2.2409887313842773\nEpoch[14001/20000] ---- VALIDATION LOSS = 13828.031689882278 --- VALIDATION ACC = 0.3221110804592915\nEpoch[14101/20000] ---- Training loss = 2.446855306625366\nEpoch[14201/20000] ---- Training loss = 2.4288477897644043\nEpoch[14301/20000] ---- Training loss = 2.4050381183624268\nEpoch[14401/20000] ---- Training loss = 2.3488292694091797\nEpoch[14501/20000] ---- Training loss = 2.3083369731903076\nEpoch[14501/20000] ---- VALIDATION LOSS = 13823.716687440872 --- VALIDATION ACC = 0.3221110804592915\nEpoch[14601/20000] ---- Training loss = 2.3241732120513916\nEpoch[14701/20000] ---- Training loss = 2.363614082336426\nEpoch[14801/20000] ---- Training loss = 2.251267671585083\nEpoch[14901/20000] ---- Training loss = 2.3068337440490723\nEpoch[15001/20000] ---- Training loss = 2.2748615741729736\nEpoch[15001/20000] ---- VALIDATION LOSS = 13820.016204833984 --- VALIDATION ACC = 0.3221110804592915\nEpoch[15101/20000] ---- Training loss = 2.215961456298828\nEpoch[15201/20000] ---- Training loss = 2.3776965141296387\nEpoch[15301/20000] ---- Training loss = 2.3045654296875\nEpoch[15401/20000] ---- Training loss = 2.3640170097351074\nEpoch[15501/20000] ---- Training loss = 2.2521767616271973\nEpoch[15501/20000] ---- VALIDATION LOSS = 13817.51556289196 --- VALIDATION ACC = 0.3221110804592915\nEpoch[15601/20000] ---- Training loss = 2.3534626960754395\nEpoch[15701/20000] ---- Training loss = 2.3377559185028076\nEpoch[15801/20000] ---- Training loss = 2.3782622814178467\nEpoch[15901/20000] ---- Training loss = 2.226295232772827\nEpoch[16001/20000] ---- Training loss = 2.3661341667175293\nEpoch[16001/20000] ---- VALIDATION LOSS = 13815.992290854454 --- VALIDATION ACC = 0.321751933667149\nEpoch[16101/20000] ---- Training loss = 2.3031489849090576\nEpoch[16201/20000] ---- Training loss = 2.35720157623291\nEpoch[16301/20000] ---- Training loss = 2.265329360961914\nEpoch[16401/20000] ---- Training loss = 2.332508087158203\nEpoch[16501/20000] ---- Training loss = 2.3699748516082764\nEpoch[16501/20000] ---- VALIDATION LOSS = 13812.83928656578 --- VALIDATION ACC = 0.32211628548526455\nEpoch[16601/20000] ---- Training loss = 2.4166595935821533\nEpoch[16701/20000] ---- Training loss = 2.2720706462860107\nEpoch[16801/20000] ---- Training loss = 2.397602081298828\nEpoch[16901/20000] ---- Training loss = 2.245281219482422\nEpoch[17001/20000] ---- Training loss = 2.3585095405578613\nEpoch[17001/20000] ---- VALIDATION LOSS = 13809.165964365005 --- VALIDATION ACC = 0.32211628548526455\nEpoch[17101/20000] ---- Training loss = 2.2564549446105957\nEpoch[17201/20000] ---- Training loss = 2.3363449573516846\nEpoch[17301/20000] ---- Training loss = 2.2349188327789307\nEpoch[17401/20000] ---- Training loss = 2.3074049949645996\nEpoch[17501/20000] ---- Training loss = 2.37516713142395\nEpoch[17501/20000] ---- VALIDATION LOSS = 13806.482866764069 --- VALIDATION ACC = 0.32211628548526455\nEpoch[17601/20000] ---- Training loss = 2.372410774230957\nEpoch[17701/20000] ---- Training loss = 2.1790695190429688\nEpoch[17801/20000] ---- Training loss = 2.3089215755462646\nEpoch[17901/20000] ---- Training loss = 2.209660768508911\nEpoch[18001/20000] ---- Training loss = 2.3889896869659424\nEpoch[18001/20000] ---- VALIDATION LOSS = 13806.427370429039 --- VALIDATION ACC = 0.32217354077096844\nEpoch[18101/20000] ---- Training loss = 2.4237782955169678\nEpoch[18201/20000] ---- Training loss = 2.2427096366882324\nEpoch[18301/20000] ---- Training loss = 2.3525772094726562\nEpoch[18401/20000] ---- Training loss = 2.2629752159118652\nEpoch[18501/20000] ---- Training loss = 2.3316774368286133\nEpoch[18501/20000] ---- VALIDATION LOSS = 13804.55996954441 --- VALIDATION ACC = 0.32217354077096844\nEpoch[18601/20000] ---- Training loss = 2.3721868991851807\nEpoch[18701/20000] ---- Training loss = 2.3424341678619385\nEpoch[18801/20000] ---- Training loss = 2.299722909927368\nEpoch[18901/20000] ---- Training loss = 2.2905826568603516\nEpoch[19001/20000] ---- Training loss = 2.343169689178467\nEpoch[19001/20000] ---- VALIDATION LOSS = 13804.483849167824 --- VALIDATION ACC = 0.3215020924204412\nEpoch[19101/20000] ---- Training loss = 2.238914728164673\nEpoch[19201/20000] ---- Training loss = 2.3699657917022705\nEpoch[19301/20000] ---- Training loss = 2.3119142055511475\nEpoch[19401/20000] ---- Training loss = 2.387794256210327\nEpoch[19501/20000] ---- Training loss = 2.2561235427856445\nEpoch[19501/20000] ---- VALIDATION LOSS = 13803.914198637009 --- VALIDATION ACC = 0.32217354077096844\nEpoch[19601/20000] ---- Training loss = 2.291675329208374\nEpoch[19701/20000] ---- Training loss = 2.2883822917938232\nEpoch[19801/20000] ---- Training loss = 2.337491989135742\nEpoch[19901/20000] ---- Training loss = 2.300612211227417\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"print(decode(bgm.generate(idx = torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=1000)[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:42:57.976066Z","iopub.execute_input":"2025-05-13T17:42:57.976345Z","iopub.status.idle":"2025-05-13T17:42:58.236027Z","shell.execute_reply.started":"2025-05-13T17:42:57.976324Z","shell.execute_reply":"2025-05-13T17:42:58.235395Z"}},"outputs":[{"name":"stdout","text":"\nUgaltaire Bare \"Hed te fr toacolike they, e She s m, ts t ink elee then che. aband wonoon The g pat frthowale tr wanike thenntie ste lll's e he enoulanysood \" h towathetha w umad wis, uly ars fogomithe at t bouno the wito he, shedee cld wabldofupetcamy?\" gopp!\" t tithas le topy ho Hedan thano the heoke befoorivenghe wost to bey!\"Liman g hey. prasoumook fllad sm! ase wasacad. sotofriton t thrflave. vike. wamad pperm ay's hed mery's waple them. s brset, wend soy uplds. Ithe Shand t wencin meund vie. havey, che o Tivye Thand cito choo.\" se lelo Shillthaf h min wayoper d. teeryoo we berede ithe p ad w â€ ber toor l. tt hedaine tt ind aie steckutheruda tend'do w th toppp alin ad vediedd he he he sert enke He The her tsatthe siethand. om vey f sath s wat f fe the dere. Ithelend ke \nThed.\n\nOnen, uttuthedom s s uthervo whed d t s canthe Thili! as a bim. meltediewaitisersorcong Soo lfftr a ay ba h walisen ateys be spe t ver \"I˜”â€ thter tht towod, as He hushe g hese t avess athernepinghe nttt.\n\n","output_type":"stream"}],"execution_count":68},{"cell_type":"markdown","source":"### Self-Attention Tested","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(1337)\nB,T,vocab_size = 4,8,32 # batch, time, vocab_size\nx = torch.randn(B,T,vocab_size)\n\n# single-head self-attention\nhead_size = 16\nkey = nn.Linear(vocab_size, head_size, bias=False)\nquery = nn.Linear(vocab_size, head_size, bias=False)\nvalue = nn.Linear(vocab_size, head_size, bias=False)\nk = key(x)   # (B, T, 16)\nq = query(x) # (B, T, 16)\nwei =  q @ k.transpose(-2, -1) # (B, T, 16) . (B, 16, T) = (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n\nout.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:43:05.055387Z","iopub.execute_input":"2025-05-13T17:43:05.056106Z","iopub.status.idle":"2025-05-13T17:43:05.136095Z","shell.execute_reply.started":"2025-05-13T17:43:05.056062Z","shell.execute_reply":"2025-05-13T17:43:05.135429Z"}},"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8, 16])"},"metadata":{}}],"execution_count":69},{"cell_type":"code","source":"wei[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:43:07.896065Z","iopub.execute_input":"2025-05-13T17:43:07.896627Z","iopub.status.idle":"2025-05-13T17:43:07.902274Z","shell.execute_reply.started":"2025-05-13T17:43:07.896599Z","shell.execute_reply":"2025-05-13T17:43:07.901524Z"}},"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n       grad_fn=<SelectBackward0>)"},"metadata":{}}],"execution_count":70},{"cell_type":"markdown","source":"# Building GPT","metadata":{}},{"cell_type":"code","source":"vocab_size = 89","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:43:08.248569Z","iopub.execute_input":"2025-05-13T17:43:08.249137Z","iopub.status.idle":"2025-05-13T17:43:08.252407Z","shell.execute_reply.started":"2025-05-13T17:43:08.249110Z","shell.execute_reply":"2025-05-13T17:43:08.251825Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"eval_interval = 500\nepochs = 5000\neval_iters = 200\nn_emb = 384\nnum_heads = 6\nnum_layers = 6\ndropout = 0.2\nblock_size = 8\nbatch_size = 32","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:52:15.959377Z","iopub.execute_input":"2025-05-13T17:52:15.959654Z","iopub.status.idle":"2025-05-13T17:52:15.963680Z","shell.execute_reply.started":"2025-05-13T17:52:15.959633Z","shell.execute_reply":"2025-05-13T17:52:15.963140Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"class SelfAttentionHead(nn.Module):\n    \"\"\"Single head for self-attention\"\"\"\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_emb, head_size, bias = False)\n        self.query = nn.Linear(n_emb, head_size, bias = False)\n        self.value = nn.Linear(n_emb, head_size, bias = False)\n\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # to save alongside model parameters\n        # buffer --> gradient doesn't flow through it\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # x = (B, T, n_emb) --> not necessarily vocab_size now\n        B, T, n_emb = x.shape\n        k = self.key(x) # (B, T, head_size)\n        q = self.query(x) # (B, T, head_size)\n\n        # we want to now calculate the attention scores (affinities of each query vector with each key vector)\n        attn = q@k.transpose(-2,-1) * k.shape[-1]**(-0.5) # (B, T, head_size) . (B, head_size, T) = (B, T, T)\n        # masking\n        masked_attn = attn.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n\n        # we have seen that k.shape[-1] == q.shape[-1] == T is not always necessarily == head_size, which is why we code it the way we have\n\n        # softmaxxing gives probability distribution\n        prob_masked_attn = F.softmax(masked_attn, dim = -1) # (B, T == each query, T == probability dist over each key)\n\n        # applying dropout\n        prob_masked_attn = self.dropout(prob_masked_attn)\n\n        # now we need to perform weighted aggregation of the values\n        v = self.value(x) # (B, T, head_size)\n        out = prob_masked_attn @ v # (B, T, T) . (B, T, head_size) = (B, T, head_size)\n        return out\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:52:16.080207Z","iopub.execute_input":"2025-05-13T17:52:16.080800Z","iopub.status.idle":"2025-05-13T17:52:16.086922Z","shell.execute_reply.started":"2025-05-13T17:52:16.080777Z","shell.execute_reply":"2025-05-13T17:52:16.086170Z"}},"outputs":[],"execution_count":91},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    \"\"\"Multiple self-attention heads in parallel\"\"\"\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([SelfAttentionHead(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size*num_heads, n_emb)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # x = (B, T, n_emb)\n        # we want to get the output from all the heads and concatenate along the feature dimension\n        out = torch.cat([head(x) for head in self.heads], dim=-1) # (B, T, num_heads * head_size)\n        out = self.proj(out)  # project back to n_emb\n        out = self.dropout(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:52:16.228624Z","iopub.execute_input":"2025-05-13T17:52:16.229354Z","iopub.status.idle":"2025-05-13T17:52:16.234311Z","shell.execute_reply.started":"2025-05-13T17:52:16.229328Z","shell.execute_reply":"2025-05-13T17:52:16.233565Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    \"\"\"Feed forward after the Multi-head Attn\"\"\"\n    def __init__(self, n_emb):\n        super().__init__()\n        self.ff = nn.Sequential(\n            nn.Linear(n_emb, 4*n_emb), \n            nn.ReLU(),\n            nn.Linear(4*n_emb, n_emb),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, x):\n        return self.ff(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:52:16.359889Z","iopub.execute_input":"2025-05-13T17:52:16.360158Z","iopub.status.idle":"2025-05-13T17:52:16.364439Z","shell.execute_reply.started":"2025-05-13T17:52:16.360137Z","shell.execute_reply":"2025-05-13T17:52:16.363855Z"}},"outputs":[],"execution_count":93},{"cell_type":"code","source":"class Block(nn.Module):\n    \"\"\"Full transformer decoder block used in our GPT implementation\"\"\"\n    def __init__(self, n_emb, num_heads):\n        super().__init__()\n        head_size = n_emb // num_heads\n        self.selfattn = MultiHeadAttention(num_heads, head_size)\n        self.ff = FeedForward(n_emb)\n        self.ln1 = nn.LayerNorm(n_emb)\n        self.ln2 = nn.LayerNorm(n_emb)\n\n    def forward(self, x):\n        x = x + self.selfattn(self.ln1(x))  # Residual + Self-attention\n        x = x + self.ff(self.ln2(x))  # Residual + FeedForward\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:52:18.499244Z","iopub.execute_input":"2025-05-13T17:52:18.499747Z","iopub.status.idle":"2025-05-13T17:52:18.504407Z","shell.execute_reply.started":"2025-05-13T17:52:18.499721Z","shell.execute_reply":"2025-05-13T17:52:18.503709Z"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":"class GPTModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding = nn.Embedding(vocab_size, n_emb) # same as token_embedding_table\n        self.position_embedding = nn.Embedding(block_size, n_emb) # we can say position_embedding_table\n\n        self.blocks = nn.Sequential(\n            *[\n                Block(n_emb, num_heads) for _ in range(num_layers)\n            ]\n        )\n\n        self.ln_f = nn.LayerNorm(n_emb) # final layer norm\n        self.head = nn.Linear(n_emb, vocab_size) # final projection to logits\n\n        self.block_size = block_size\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # check for the input length\n        assert T <= self.block_size, f\"Cannot forward, sequence length {T} exceeds block size {self.block_size}\"\n\n        token_embed = self.token_embedding(idx) # (B, T, n_emb)\n        position_embed = self.position_embedding(torch.arange(T, device = device)) # (T, n_emb)\n        x = token_embed + position_embed # (B, T, n_emb)\n        x = self.blocks(x) # (B, T, n_emb)\n        x = self.ln_f(x) # (B, T, n_emb)\n        logits = self.head(x) # (B, T, vocab_size)\n\n        if targets is None:\n            loss = None\n\n        else:\n            B, T, vocab_size = logits.shape\n            logits = logits.view(B*T, vocab_size)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -self.block_size:] # crop to block_size\n            logits, _ = self(idx_cond)\n            logits = logits[:,-1,:] # # (B, vocab_size) - last token logits\n            probs = F.softmax(logits, dim=-1)\n            next_token = torch.multinomial(probs, num_samples=1) # (B, 1)\n            idx = torch.cat((idx, next_token), dim=1) # (B, T+1)\n        return idx\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:52:18.639420Z","iopub.execute_input":"2025-05-13T17:52:18.639927Z","iopub.status.idle":"2025-05-13T17:52:18.647402Z","shell.execute_reply.started":"2025-05-13T17:52:18.639903Z","shell.execute_reply":"2025-05-13T17:52:18.646721Z"}},"outputs":[],"execution_count":95},{"cell_type":"code","source":"model = GPTModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:52:20.772074Z","iopub.execute_input":"2025-05-13T17:52:20.772365Z","iopub.status.idle":"2025-05-13T17:52:20.911043Z","shell.execute_reply.started":"2025-05-13T17:52:20.772343Z","shell.execute_reply":"2025-05-13T17:52:20.910444Z"}},"outputs":[{"name":"stdout","text":"10.712153 M parameters\n","output_type":"stream"}],"execution_count":96},{"cell_type":"code","source":"print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=100)[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:52:23.391826Z","iopub.execute_input":"2025-05-13T17:52:23.392655Z","iopub.status.idle":"2025-05-13T17:52:24.584352Z","shell.execute_reply.started":"2025-05-13T17:52:23.392625Z","shell.execute_reply":"2025-05-13T17:52:24.583598Z"}},"outputs":[{"name":"stdout","text":"\nc\n?:YtubA-dZH'Wd”St2'ÂWu,'U±qWdl“y;¦:edte“6“AgO€B?6lÂ23K2'v4ISBâ™ULbD³NœoEoMeX6LSVbif;Ã0qqme-ZuSâ?CT\n","output_type":"stream"}],"execution_count":97},{"cell_type":"code","source":"train_losses, val_losses = [], []\ntrain_accuracies, val_accuracies = [], []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:52:26.942540Z","iopub.execute_input":"2025-05-13T17:52:26.942823Z","iopub.status.idle":"2025-05-13T17:52:26.946428Z","shell.execute_reply.started":"2025-05-13T17:52:26.942803Z","shell.execute_reply":"2025-05-13T17:52:26.945863Z"}},"outputs":[],"execution_count":98},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:52:27.924155Z","iopub.execute_input":"2025-05-13T17:52:27.924618Z","iopub.status.idle":"2025-05-13T17:52:27.930454Z","shell.execute_reply.started":"2025-05-13T17:52:27.924595Z","shell.execute_reply":"2025-05-13T17:52:27.929617Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"step = 0\nmodel.train()\npbar = tqdm(total=epochs)\ntrain_iter = iter(train_loader)\n\nwhile step < epochs:\n    try:\n        xb, yb = next(train_iter)\n    except StopIteration:\n        train_iter = iter(train_loader)\n        xb, yb = next(train_iter)\n\n    xb, yb = xb.to(device), yb.to(device)\n    # print(xb.shape, yb.shape)\n    logits, loss = model(xb, yb)\n    if(step%100 == 0):\n        print(f\"Epoch[{step+1}/{epochs}] ---- Training loss = {loss}\")\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # record training metrics every eval_interval steps\n    if step % eval_interval == 0:\n        model.eval()\n        with torch.no_grad():\n            # training eval\n            # print(\"Training eval\")\n            train_logits, _ = model(xb, yb)\n            train_acc = compute_accuracy(train_logits.view(-1, vocab_size), yb.view(-1)).item()\n            train_losses.append(loss.item())\n            train_accuracies.append(train_acc)\n\n            #  validation eval\n            # print(\"Validation eval\")\n            val_loss_total, val_correct, val_total = 0.0, 0, 0\n            for val_xb, val_yb in val_loader:\n                val_xb, val_yb = val_xb.to(device), val_yb.to(device)\n                val_logits, val_loss = model(val_xb, val_yb)\n                val_loss_total += val_loss.item()\n                \n                preds = val_logits.view(-1, vocab_size).argmax(dim=-1)  # shape: (B*T,)\n                targets = val_yb.view(-1)  # shape: (B*T,)\n                val_correct += (preds == targets).sum().item()\n                val_total += val_yb.numel()\n            \n            val_losses.append(val_loss_total / len(val_loader))\n            val_accuracies.append(val_correct / val_total)\n            print(f\"Epoch[{step+1}/{epochs}] ---- VALIDATION LOSS = {val_loss_total} --- VALIDATION ACC = {val_correct/val_total}\")\n        model.train()\n    \n    step += 1\n    pbar.update(1)\n\npbar.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:56:21.142162Z","iopub.execute_input":"2025-05-13T17:56:21.142875Z","iopub.status.idle":"2025-05-13T18:11:24.465601Z","shell.execute_reply.started":"2025-05-13T17:56:21.142849Z","shell.execute_reply":"2025-05-13T18:11:24.464899Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5971306129f24becbfa15eeff94ce426"}},"metadata":{}},{"name":"stdout","text":"Epoch[1/5000] ---- Training loss = 3.0992045402526855\nEpoch[1/5000] ---- VALIDATION LOSS = 18190.350291252136 --- VALIDATION ACC = 0.24980546215425614\nEpoch[101/5000] ---- Training loss = 2.321361780166626\nEpoch[201/5000] ---- Training loss = 2.247704029083252\nEpoch[301/5000] ---- Training loss = 2.0146918296813965\nEpoch[401/5000] ---- Training loss = 1.9151442050933838\nEpoch[501/5000] ---- Training loss = 1.9007267951965332\nEpoch[501/5000] ---- VALIDATION LOSS = 11237.518462061882 --- VALIDATION ACC = 0.4446614651107109\nEpoch[601/5000] ---- Training loss = 1.766221284866333\nEpoch[701/5000] ---- Training loss = 1.8081618547439575\nEpoch[801/5000] ---- Training loss = 1.7742477655410767\nEpoch[901/5000] ---- Training loss = 1.9560261964797974\nEpoch[1001/5000] ---- Training loss = 1.671762466430664\nEpoch[1001/5000] ---- VALIDATION LOSS = 10500.084830343723 --- VALIDATION ACC = 0.473748451504773\nEpoch[1101/5000] ---- Training loss = 1.665525197982788\nEpoch[1201/5000] ---- Training loss = 1.7948825359344482\nEpoch[1301/5000] ---- Training loss = 1.9623409509658813\nEpoch[1401/5000] ---- Training loss = 1.7390995025634766\nEpoch[1501/5000] ---- Training loss = 1.56982421875\nEpoch[1501/5000] ---- VALIDATION LOSS = 10254.263776421547 --- VALIDATION ACC = 0.4827746171703397\nEpoch[1601/5000] ---- Training loss = 1.7842501401901245\nEpoch[1701/5000] ---- Training loss = 1.7709589004516602\nEpoch[1801/5000] ---- Training loss = 1.5885086059570312\nEpoch[1901/5000] ---- Training loss = 1.6713926792144775\nEpoch[2001/5000] ---- Training loss = 1.6333063840866089\nEpoch[2001/5000] ---- VALIDATION LOSS = 9948.92204618454 --- VALIDATION ACC = 0.495357116832013\nEpoch[2101/5000] ---- Training loss = 1.7641657590866089\nEpoch[2201/5000] ---- Training loss = 1.5274354219436646\nEpoch[2301/5000] ---- Training loss = 1.6367650032043457\nEpoch[2401/5000] ---- Training loss = 1.6005216836929321\nEpoch[2501/5000] ---- Training loss = 1.6790953874588013\nEpoch[2501/5000] ---- VALIDATION LOSS = 9813.86231803894 --- VALIDATION ACC = 0.499686397185122\nEpoch[2601/5000] ---- Training loss = 1.492321491241455\nEpoch[2701/5000] ---- Training loss = 1.54281747341156\nEpoch[2801/5000] ---- Training loss = 1.651579737663269\nEpoch[2901/5000] ---- Training loss = 1.7090028524398804\nEpoch[3001/5000] ---- Training loss = 1.8651951551437378\nEpoch[3001/5000] ---- VALIDATION LOSS = 9713.287926793098 --- VALIDATION ACC = 0.506284418234247\nEpoch[3101/5000] ---- Training loss = 1.8787269592285156\nEpoch[3201/5000] ---- Training loss = 1.654736876487732\nEpoch[3301/5000] ---- Training loss = 1.5806019306182861\nEpoch[3401/5000] ---- Training loss = 1.6188573837280273\nEpoch[3501/5000] ---- Training loss = 1.6031988859176636\nEpoch[3501/5000] ---- VALIDATION LOSS = 9570.886499762535 --- VALIDATION ACC = 0.511638438075806\nEpoch[3601/5000] ---- Training loss = 1.7160524129867554\nEpoch[3701/5000] ---- Training loss = 1.6676689386367798\nEpoch[3801/5000] ---- Training loss = 1.6252590417861938\nEpoch[3901/5000] ---- Training loss = 1.4978668689727783\nEpoch[4001/5000] ---- Training loss = 1.3775913715362549\nEpoch[4001/5000] ---- VALIDATION LOSS = 9527.66724127531 --- VALIDATION ACC = 0.5107731025077815\nEpoch[4101/5000] ---- Training loss = 1.6595022678375244\nEpoch[4201/5000] ---- Training loss = 1.5138167142868042\nEpoch[4301/5000] ---- Training loss = 1.6882089376449585\nEpoch[4401/5000] ---- Training loss = 1.6383461952209473\nEpoch[4501/5000] ---- Training loss = 1.7305253744125366\nEpoch[4501/5000] ---- VALIDATION LOSS = 9406.47798615694 --- VALIDATION ACC = 0.5155090255150373\nEpoch[4601/5000] ---- Training loss = 1.6290339231491089\nEpoch[4701/5000] ---- Training loss = 1.7893881797790527\nEpoch[4801/5000] ---- Training loss = 1.4296609163284302\nEpoch[4901/5000] ---- Training loss = 1.6566969156265259\n","output_type":"stream"}],"execution_count":104},{"cell_type":"code","source":"vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:24:43.248777Z","iopub.status.idle":"2025-05-13T17:24:43.249629Z","shell.execute_reply.started":"2025-05-13T17:24:43.249447Z","shell.execute_reply":"2025-05-13T17:24:43.249465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=1000)[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:12:24.567609Z","iopub.execute_input":"2025-05-13T18:12:24.568125Z","iopub.status.idle":"2025-05-13T18:12:36.446311Z","shell.execute_reply.started":"2025-05-13T18:12:24.568101Z","shell.execute_reply":"2025-05-13T18:12:36.445696Z"}},"outputs":[{"name":"stdout","text":"\nWick your chan birk an a sealh. \n\nMack and spart.\" Timmy was dot it laughed to looked. She home, bear when. And special car cound at y, Lily lamped in amine home to make about tok.\n\nBentink there,\" said, \"You're diden. Sammy, when and with a griends. They were give and take happy tooks everyone was so the both and the latten shark the trake.\n\nSam loods to ust is a big back yarty to the smiled and was day, they skide. He want not with arroway pick, busy - calloof the adn't dreamp. Timmy ater someone was a didn't lyook, but the was he cattant the decom. Lily and him was sâ€mined his fish, that some you?\"\n\nDaid notiched ever fast and spedful was may well boaves courn, find let you, Mury lated to go look to smaly that had like her do coki, \"That secaree was friend scade and save limbith'. Her mom was a maide Bob?\"\n\nMummy ater. \"Sammy in nevery like fun. Tom to to the duck anythings found eat playing like to trucky it corning!\" Beily looked are was a liked the sparthing they were pall. He s\n","output_type":"stream"}],"execution_count":106},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}